# Steps to process data

## Generate catalog of files to process

Create a file detailing all files we know about which could be processed.

```bash
path_to_files=/hps/path/to/gffs
target_files="target.files"
find $path_to_files -name '*.gff.gz' > $target_files
```

## Generate sbatch scripts

Generate an sbatch and use a previous run to filter what we're going to run on.

```bash
workdir=/path/to/work
email=email@ebi.ac.uk
target_batch='amr_parse.sbatch'
previously_processed=${workdir}/previously-processed.files
python3 generate_sbatch.py \
  --base-dir $workdir \
  --email $email \
  --to-process $target_files \
  --output $target_batch \
  --memory 16GB \
  --previously-processed $previously_processed
```

The script generates an sbatch, a set of split files (filtered for those files we've already processed) and creates a job array for each split file. It will also create the following directories.

- `logs`
- `parsed`
- `parsed/genotype`
- `parsed/genotype/parquet`
- `parsed/phenotype`
- `parsed/assembly`

To submit run

```bash
sbatch $target_batch
```

## Creating the parquet files

To create the parquet files we merge the CSVs and then use pyarrow to convert. Using a schema ensures data integrity.

```bash
AMR_SOFTWARE=$PWD
python3 ${AMR_SOFTWARE}/convert_and_merge_csv_to_parquet.py \
  --input_dir ${workdir}/parsed/genotype \
  --pattern 'genotype.*.csv' \
  --output_dir ${workdir}/parsed/genotype/parquet \
  --merged_file ${workdir}/parsed/genotype/genotype.merged.parquet \
  --schema_file ${AMR_SOFTWARE}/schemas/genotype.schema.json
```

## Edits to the phenotype file

### Adding country and regions

Only run this command if the phenotype file doesn't already have the country and regions from the ISO code populated.

```bash
phenotype_parquet_no_countries=${workdir}/parsed/phenotype/phenotype.parquet
fixed_phenotype_parquet=${workdir}/parsed/phenotype/phenotype.countries.parquet
$AMR_SOFTWARE/add_country_from_country_code.py \
  --input $phenotype_parquet_no_countries \
  --output $fixed_phenotype_parquet \
  --country-codes $AMR_SOFTWARE/configs/country-codes.csv \
  --unsd $AMR_SOFTWARE/configs/unsd-country.csv \
  --iso-code-column ISO_country_code
```

### Combining previous runs: via parquet

If you have previous runs you can either create the new parquet file by copying over previous results or merging the two resulting parquet files. The following does this for parquet formatted files.

```bash
previous_parquet='genotype.previous.parquet'
merged_file=${workdir}/parsed/genotype.merged.prior.parquet
$AMR_SOFTWARE/stream_merge_parquet.py \
  --files ${workdir}/parsed/genotype/genotype.merged.parquet $previous_parquet \
  --output_file ${workdir}/parsed/genotype.merged.prior.parquet
```

Note merging will not work if they are different in their schema composition.

## Post fixing

In order to apply final fixes to the files you need to run the `post_fixes.py` script on both genotype and phenotype parquet files. For phenotype this will:

- Fix a known BioSample_ID error
- Drop any autogenerated columns (those starting with `gen_`)
- Drop the `anitbiotic_abbreviation` column

In genotype this will:

- Replace species names with those from `phenotype` where BioSample_ID is the same
- Normalise species names to just the first part (removing anything like `subsp.`, `complex` or `variant`)
- Delete any rows where `taxon_id` is `NULL`
- Add any missing antibiotic names
- Set any antibiotic to `NULL` where it is set to `''`
- Drop any autogenerated columns (those starting with `gen_`)
- Drop the `anitbiotic_abbreviation` column
- Delete any suppressed genomes

```bash
phenotype_file=phenotype.parquet
$AMR_SOFTWARE/post_fixes.py \
  --genotype $merged_file \
  --phenotype $phenotype_file \
  --output-dir $workdir \
  --antibiotic-lookup $AMR_SOFTWARE/configs/fix-antibiotics.csv \
  --filter-genomes $AMR_SOFTWARE/configs/filter-files/assembly_qc_filter.csv
```

## Creating the merged dataset

```bash
cd $workdir
$AMR_SOFTWARE/join_parquet.py \
  --config $AMR_SOFTWARE/configs/merge_pheno_geno.yaml
```

This will create a new file for the merged data set (based on BioSampleID, assembly_ID and antibiotic) in the workdir. If you cd to the workdir then there should be no reason to change the config file.

## Creating CSVs

```bash
for file in $workdir/*.parquet; do
    echo "Writing $file to compressed CSV"
    $AMR_SOFTWARE/parquet_to_csv_gz.py $file ${file%.parquet}.csv.gz
done
```
